{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention  \n",
    "由google提出，Attention is all your need. 此模型结合了RNN(考虑全序列)和CNN(并行处理)的优点。   \n",
    "目标：学会使用Transformer.   \n",
    "## 任务描述   \n",
    "- 根据给定的特征对说话者进行分类.  \n",
    "## 模型描述  \n",
    "# Model\n",
    "- TransformerEncoderLayer:\n",
    "  - Base transformer encoder layer in [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "  - Parameters:\n",
    "    - d_model: input的特征维数（必须设置）.   \n",
    "\n",
    "    - nhead: Self-Attention模型中multihead的数量(必须设置).  \n",
    "\n",
    "    - dim_feedforward: 前馈网络模型的维数 (default=2048).\n",
    "\n",
    "    - dropout: the dropout value (default=0.1).\n",
    "\n",
    "    - activation: 神经网络的激活函数 relu or gelu (default=relu).\n",
    "\n",
    "- TransformerEncoder:\n",
    "  - TransformerEncoder is a stack of N transformer encoder layers\n",
    "  - Parameters:\n",
    "    - encoder_layer: 一个 TransformerEncoderLayer() 的实例(必须设置).\n",
    "\n",
    "    - num_layers: 编码器中的子编码器层数 (必须设置).\n",
    "\n",
    "    - norm: 层Normal化组件 (必须设置)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "  def __init__(self, d_model=80, n_spks=600, dropout=0.1):\n",
    "    super().__init__()\n",
    "    self.prenet = nn.Linear(40, d_model)\n",
    "    self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "      d_model=d_model, dim_feedforward=256, nhead=2\n",
    "    )\n",
    "    self.pred_layer = nn.Sequential(\n",
    "      nn.Linear(d_model, d_model),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(d_model, n_spks),\n",
    "    )\n",
    "\n",
    "  def forward(self, mels):\n",
    "    \"\"\"\n",
    "    args:\n",
    "      mels: (batch size, length, 40)\n",
    "    return:\n",
    "      out: (batch size, n_spks)\n",
    "    \"\"\"\n",
    "    out = self.prenet(mels)\n",
    "    out = out.permute(1, 0, 2)\n",
    "    out = self.encoder_layer(out)\n",
    "    out = out.transpose(0, 1)\n",
    "    stats = out.mean(dim=1)\n",
    "    out = self.pred_layer(stats)\n",
    "    return out\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
