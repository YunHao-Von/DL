{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3-图像分类  \n",
    "## <font color=green> 目标:  \n",
    "(1)通过卷积神经网络模型来搭建图像分类模型。  \n",
    "(2)通过数据拓展来提升模型效果。  \n",
    "(3)了解如何利用未标记的数据以及如何从中受益。  \n",
    "</font>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 食物分类:  \n",
    "![avatar](pvg/p1.png)  \n",
    "这些图像来自于food-11数据集，所有的数据被划分为11类。   \n",
    "这里的数据集略有修改   \n",
    "训练集:280 * 11标记图像+ 6786未标记图像  \n",
    "验证集:30 * 11标记图像   \n",
    "测试集:3347图像   \n",
    "不要使用原始数据集或标签"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.导入依赖包  \n",
    "本次项目要高度依赖TorchVision,可以将其视为一个依赖于Torch的视觉模型专用包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Dataset, Data Loader, and Transforms  \n",
    "Torchvision提供了大量关于图像处理的工具，比如数据打包和数据拓展等。  \n",
    "本次图像存储在标签文件夹中，我们可以直接通过这些包来打包数据。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfm = transforms.Compose([\n",
    "    transforms.Resize((128,128)),  # 将图片转化为128*128像素的图片\n",
    "    transforms.ToTensor(),  # 将图片向量转化为张量\n",
    "])\n",
    "test_tfm = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "batch_size = 128\n",
    "\n",
    "train_set = DatasetFolder(\"data/training\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=train_tfm)\n",
    "valid_set = DatasetFolder(\"data/validation\", loader=lambda x: Image.open(x), extensions=\"jpg\", transform=test_tfm)\n",
    "\n",
    "# Construct data loaders.\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True,  pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        # The arguments for commonly used modules:\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "\n",
    "        # input image size: [3, 128, 128]\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4, 0),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 11)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input (x): [batch_size, 3, 128, 128]\n",
    "        # output: [batch_size, 11]\n",
    "\n",
    "        # Extract features by convolutional layers.\n",
    "        x = self.cnn_layers(x)\n",
    "\n",
    "        # The extracted feature map must be flatten before going to fully-connected layers.\n",
    "        x = x.flatten(1)\n",
    "\n",
    "        # The features are transformed by fully-connected layers to obtain the final logits.\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b0d5fec0d747558db0faa09c622206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 001/080 ] loss = 1.99787, acc = 0.29537\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881dc9b185954f81947980aa16bdc433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 002/080 ] loss = 1.64149, acc = 0.43680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9101633fd4f4b7aa94c72634a63e253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 003/080 ] loss = 1.45473, acc = 0.49429\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a27183bd28140708335cb082d1143c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 004/080 ] loss = 1.30148, acc = 0.54497\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6920df9e468d47a483fdd5af894c1ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Train | 005/080 ] loss = 1.18796, acc = 0.58578\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b772d46b4a8a4ba3806dbe6a62a4f5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize a model, and put it on the device specified.\n",
    "model = Classifier().to(device)\n",
    "model.device = device\n",
    "\n",
    "# For the classification task, we use cross-entropy as the measurement of performance.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)\n",
    "\n",
    "# The number of training epochs.\n",
    "n_epochs = 80\n",
    "\n",
    "# Whether to do semi-supervised learning.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # ---------- TODO ----------\n",
    "    # In each epoch, relabel the unlabeled dataset for semi-supervised learning.\n",
    "    # Then you can combine the labeled dataset and pseudo-labeled dataset for the training.\n",
    "    # ---------- Training ----------\n",
    "    # Make sure the model is in train mode before training.\n",
    "    model.train()\n",
    "\n",
    "    # These are used to record information in training.\n",
    "    train_loss = []\n",
    "    train_accs = []\n",
    "\n",
    "    # Iterate the training set by batches.\n",
    "    for batch in tqdm(train_loader):\n",
    "\n",
    "        # A batch consists of image data and corresponding labels.\n",
    "        imgs, labels = batch\n",
    "\n",
    "        # Forward the data. (Make sure data and model are on the same device.)\n",
    "        logits = model(imgs.to(device))\n",
    "\n",
    "        # Calculate the cross-entropy loss.\n",
    "        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n",
    "        loss = criterion(logits, labels.to(device))\n",
    "\n",
    "        # Gradients stored in the parameters in the previous step should be cleared out first.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute the gradients for parameters.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradient norms for stable training.\n",
    "        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "\n",
    "        # Update the parameters with computed gradients.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the accuracy for current batch.\n",
    "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
    "\n",
    "        # Record the loss and accuracy.\n",
    "        train_loss.append(loss.item())\n",
    "        train_accs.append(acc)\n",
    "\n",
    "    # The average loss and accuracy of the training set is the average of the recorded values.\n",
    "    train_loss = sum(train_loss) / len(train_loss)\n",
    "    train_acc = sum(train_accs) / len(train_accs)\n",
    "\n",
    "    # Print the information.\n",
    "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d12a6980af1de3549060b7b451d48d445ec6b4aaeaf0b0e12a509d2182e95745"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
