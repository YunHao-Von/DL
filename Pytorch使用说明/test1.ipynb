{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch  # 引入Pytorch库\n",
    "print(torch.cuda.is_available())  # 测试GPU是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "class LogicNet(nn.Module):  #继承nn.Module类，构建网络模型\n",
    "    def __init__(self,inputdim,hiddendim,outputdim):  # 初始化函数中确定网络结构\n",
    "        super(LogicNet,self).__init__()\n",
    "        self.Linear1 = nn.Linear(inputdim,hiddendim)  # 定义全连接层\n",
    "        self.Linear2 = nn.Linear(hiddendim,outputdim)  # 定义全连接层\n",
    "        self.criterion = nn.CrossEntropyLoss()  # 定义交叉熵函数\n",
    "    \n",
    "    def forward(self,x):  # 搭建网络模型\n",
    "        x = self.Linear1(x)  # 将输入数据传入第1搁全连接层\n",
    "        x = torch.tanh(x)  # 对第1个连接层的结果进行非线性变换（使用tanh函数）\n",
    "        x = self.Linear2(x)  # 将完成了非线性变换后的数据传入第二个连接层\n",
    "        return x  # 输出第二个连接层的结果\n",
    "    \n",
    "    def predict(self,x):  # 实现LogicNet类的预测接口\n",
    "        # 调用自身网络模型，并且对结果进行softmax处理，分别得出预测数据中属于每一类的概率\n",
    "        x = self.forward(x)  # 调用自身网络模型，将结果赋到x中\n",
    "        pred = torch.softmax(x,dim=1)  # 返回每组预测概率中最大值的索引\n",
    "    \n",
    "    def getloss(self,x,y):  # 实现LogicNet类的损失值接口\n",
    "        y_pred = self.forward(x)\n",
    "        loss = self.criterion(y_pred,y)  # 计算损失值的交叉熵\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogicNet(inputdim=2,hiddendim=3,outputdim=2)  # 实例化模型\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)  # 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(feature_data).type(torch.FloatTensor)  # 将numpy数据转化为张量  \n",
    "y = torch.from_numpy(target_data).type(torch.LongTensor)\n",
    "epochs = 270  # 定义迭代次数\n",
    "losses = list()  # 定义列表，用于接受每一步的损失值\n",
    "for i in range(epochs):\n",
    "    loss = model.getloss(x,y)  # 将数据带入模型进行计算，得到损失值\n",
    "    losses.append(loss.item())  # 保留这一轮训练的损失值到losses中\n",
    "    optimizer.zero_grad()  # 清空之前的梯度\n",
    "    loss.backward()  # 反向传播损失值\n",
    "    optimizer.step()  # 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([1, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(2,1)  # 定义一个张量\n",
    "print(x.shape)  # 打印张量形状，输出:torch.Size([2,1])\n",
    "print(x.size())  # 打印张量大小，输出:torch.Size([2, 1])\n",
    "print(x.reshape([1,2]).shape)  # 先将张量的形状重新塑造，然后打印张量的形状，输出:torch.Size([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3923, -0.2236, -0.3195],\n",
       "        [-1.2050,  1.0445, -0.6332]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(2)  # 设置随机数种子  \n",
    "torch.initial_seed()  # 查看随机数种子，输出:2\n",
    "torch.randn(2,3)  # 随机生成指定形状的随机值张量\n",
    "torch.arange(1,10,step=2)  # 在1到10之间，按照步长为2进行取值，输出:tensor([1,3,5,7,9])\n",
    "torch.linspace(1,9,steps=5)  # 在1到9之间，均匀地取出5个值，输出:tensor([1.,3.,5.,7.,9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.FloatTensor([4])\n",
    "b = torch.add(a,a)  # 调用torch.add()进行相加\n",
    "torch.add(a,a,out=b)  # 调用torch.add()进行相加，并且将结果输出给b\n",
    "a.add_(b)  # 实现 a+=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "print(torch.reshape(a,(1,-1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [5, 6, 7],\n",
      "        [2, 8, 0]])\n",
      "tensor([[1, 2, 3, 5, 6, 7],\n",
      "        [4, 5, 6, 2, 8, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 2],\n",
       "         [4, 5]]),\n",
       " tensor([[3],\n",
       "         [6]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[5,6,7],[2,8,0]])\n",
    "torch.t(b)  # 转置矩阵\n",
    "torch.transpose(b,dim0=1,dim1=0)  # 转置矩阵\n",
    "b.permute(1,0)  # 将矩阵的第0维度与第1维度交换\n",
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "print(torch.cat([a,b],dim=0))  # 输出:tensor([[1, 2, 3],[4, 5, 6],[5, 6, 7],[2, 8, 0]])\n",
    "print(torch.cat([a,b],dim=1))  # 输出:tensor([[1, 2, 3, 5, 6, 7],[4, 5, 6, 2, 8, 0]])\n",
    "torch.chunk(a,chunks=2,dim=0)  # 将张量a沿着第0维度分割成2部分,返回类型是tuple\n",
    "torch.chunk(a,chunks=2,dim=1)  # 将张量a沿着第1维度分割成2部分,返回类型是tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 8, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[5,6,7],[2,8,0]])  # 定义一个二维张量\n",
    "torch.split(b,split_size_or_sections=(1,2),dim =1)  # 将张量b沿着第1维度分割成两个部分\n",
    "                                                    # 输出:(tensor([[5],[2]]),tensor([[6, 7],[8, 0]]))\n",
    "torch.split(b,split_size_or_sections=2,dim =1)  # 将张量b沿着第1维度分割成2部分，每部分按照2个元素进行拆分。不满足指定个数的剩余数据，将会被作为分割数据的最后一部分\n",
    "torch.gather(b,dim=1,index=torch.tensor([[1,0],[1,2]]))  # 沿着第1维度，按照index的形状进行取值排列。输出tensor([[6, 5],[8, 0]])\n",
    "torch.index_select(b,dim=0,index=torch.tensor(1))  # 沿着第0维度,取出第一个元素,输出:tensor([[2, 8, 0]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d12a6980af1de3549060b7b451d48d445ec6b4aaeaf0b0e12a509d2182e95745"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
